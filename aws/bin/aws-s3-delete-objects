#!/usr/bin/env bash

# AWS S3 Delete Objects command
# =============================
# Tim Friske <me@tifr.de>
#
# Deletes batches of up to 1,000 S3 objects per request via the low- level
# command `aws s3api delete-objects`. This is much faster than using the
# higher-level command `aws s3 rm` which is only able to delete one S3 object
# per invocation.
#
# Deps:: awk, aws, bash, cat, env, find, jq, less, mktemp, parallel, sh
# Deps:: sort

# Ensure fail-fast script execution.
shopt -os nounset pipefail errexit errtrace

# Standard input stream of script must be connected to terminal in order to
# prompt the user.
if [[ ! -t 0 ]]; then
  echo >&2 stdin must be connected to terminal
  exit 1
fi

# Input parameters and validation:
# AWS S3 bucket without leading schema (`s3://`).
AWS_S3_BUCKET="${1:-${AWS_S3_BUCKET:?required}}"
# Text file listing each S3 object key on separate line.
AWS_S3_OBJECT_KEY_TEXT_FILE="${2:-${AWS_S3_OBJECT_KEY_TEXT_FILE:?required}}"

# Abort if AWS S3 object key listing file is not text file.
if ! grep --binary-files=without-match --regexp=. --quiet -- "${AWS_S3_OBJECT_KEY_TEXT_FILE}"; then
  printf >&2 'file with S3 keys must be plain-text: %s\n' "${AWS_S3_OBJECT_KEY_TEXT_FILE}"
  exit 1
fi

# Create a temporary directory with the given bucket as a prefix and a random
# suffix as its name. It is used to store intermediate results such as the json
# files which are generated to list up to 1,000 S3 keys of objects to delete.
# Also GNU parallel stores its joblog file in this directory.
TEMP_DIR="$(mktemp --directory --tmpdir="${PWD}" -- "${AWS_S3_BUCKET}.XXXXXXXXXX")"
TEMP_DIR="${TEMP_DIR#"${PWD}/"}"
printf 'Temporary directory: %s\n' "${TEMP_DIR}"

# Delete blank and duplicate lines. S3 objects can be deleted only once.
awk --source='NF&&!a[$0]++' -- "${AWS_S3_OBJECT_KEY_TEXT_FILE}" > "${TEMP_DIR}/unique_s3_keys.txt"

# From the given text file with S3 keys of objects to delete generate json files
# with up to 1,000 of these S3 keys. Produce as many json files as needed in
# order to not exceed the maximum number of objects which the `aws s3api
# delete-objects` command can handle at once, i.e. per HTTP request.
parallel --max-lines 1000 --max-args 1 --keep-order --pipe -- < "${TEMP_DIR}/unique_s3_keys.txt" \
    sh -c "cat | jq --raw-input '{Key:.}' | jq --slurp '{Objects:.,Quiet:true}' > '${TEMP_DIR}/{#}.json'"

echo ==========================================================================
echo WARNING: CAREFULLY REVIEW THE S3 OBJECTS BEFORE THEY WILL BE DELETED
echo WARNING: AFTER REVIEW YOU ARE ASKED AGAIN WHETHER YOU WANT TO CONTINUE
echo ==========================================================================
printf 'Bucket: %s\n' "${AWS_S3_BUCKET}"
read -n 1 -s -r -p 'Press CTRL-C to abort or any other key to continue...'
echo

find "${TEMP_DIR}" -maxdepth 1 -type f -name '*.json' \
  | sort --sort=numeric --key=1.3,1 \
  | xargs --no-run-if-empty -- jq --raw-output '.Objects[].Key' -- \
  | less

echo ==========================================================================
echo WARNING: YOU ARE ABOUT TO DELETE THE S3 OBJECTS YOU JUST REVIEWED
echo WARNING: ARE YOU ABSOLUTELY SURE TO IRREVERSIBLY DELETE THE S3 OBJECTS
echo ==========================================================================
printf 'Bucket: %s\n' "${AWS_S3_BUCKET}"
printf 'Temporary directory: %s\n' "${TEMP_DIR}"
read -n 1 -s -r -p 'Press CTRL-C to abort or any other key to continue...'
echo

# For every previously produced json file run as many processes of the `aws
# s3api delete-objects` command in parallel as there are threads in order to
# maximize the number of S3 objects which are deleted per time unit.
find "${TEMP_DIR}" -maxdepth 1 -type f -name '*.json' \
  | sort --sort=numeric --key=1.3,1 \
  | parallel --max-args 1 --keep-order --joblog "${TEMP_DIR}/joblog" --verbose -- \
    aws s3api delete-objects --bucket "${AWS_S3_BUCKET}" --delete 'file://{}'

# Archive deletion protocol for auditing purposes in the future.
tar --create --use-compress-program='zstd --threads=0' --verbose --file="${TEMP_DIR}.tar.gz" --sort=name -- "${TEMP_DIR}"
